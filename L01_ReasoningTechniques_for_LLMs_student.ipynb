{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Reasoning Techniques for Large Language Models\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Dr Chao Shu (chao.shu@qmul.ac.uk)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Updated imports for LCEL\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to LLM Reasoning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Large Language Models (LLMs) like GPT-4, LLaMA, and others have demonstrated impressive capabilities across various tasks. However, their ability to reason through complex problems isn't inherently straightforward.\n",
    "\n",
    "**What is reasoning in LLMs?**\n",
    "- The ability to process information logically\n",
    "- Breaking down complex problems into steps\n",
    "- Making inferences based on provided context\n",
    "- Arriving at conclusions through structured thinking\n",
    "\n",
    "**Why is reasoning important?**\n",
    "- Enables solving complex problems that require multi-step thinking\n",
    "- Improves transparency of model decision-making\n",
    "- Enhances reliability and reduces hallucinations\n",
    "- Makes LLMs more useful for specialized domains (mathematics, programming, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üßë‚Äçüè´ Demo: Responses with and without Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see the difference in responses from a non-reasoning (instruction) model and a reasoning model. Both models are small-scale open-weight model running locally.\n",
    "\n",
    "ü§ñ Example Prompts:\n",
    "\n",
    "1. \"Answer the questions briefly and directly with just a few words. \n",
    "\n",
    "Richard lives in an apartment building with 15 floors. Each floor contains 8 units, and 3/4 of the building is occupied. What's the total number of unoccupied units In the building?\" (*from [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k)*)\n",
    "\n",
    "2. \"how many 'r's in 'strawberry'?\"\n",
    "\n",
    "3. \"9.7 and 9.11, which number is bigger?\"\n",
    "\n",
    "üß† Critical Thinking: What caused the differences? How the reasoning capability is developed in reasoning models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's use LangChain to demo the Q&A in a programmatical way using simplest codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Non-reasoning model response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='72', additional_kwargs={}, response_metadata={'model': 'qwen2:1.5b', 'created_at': '2025-03-06T15:17:36.857422Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5079110000, 'load_duration': 4121990542, 'prompt_eval_count': 64, 'prompt_eval_duration': 918000000, 'eval_count': 3, 'eval_duration': 36000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-57c5e6ba-5e00-4895-a244-b822e6014370-0', usage_metadata={'input_tokens': 64, 'output_tokens': 3, 'total_tokens': 67})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"\"\"Answer the questions briefly and directly with just a few words. \n",
    "Richard lives in an apartment building with 15 floors. Each floor contains 8 units, and 3/4 of the building is occupied. What's the total number of unoccupied units In the building?\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm_qwen = ChatOllama(model=\"qwen2:1.5b\", temperature=0.7)\n",
    "\n",
    "output = llm_qwen.invoke(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reasoning model response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how many unoccupied units are in Richard's apartment building. Let me break it down step by step.\n",
      "\n",
      "First, the building has 15 floors. Each floor has 8 units. So, the total number of units is 15 multiplied by 8. Let me do that: 15 * 8 equals 120. So there are 120 units in total.\n",
      "\n",
      "Now, it says that 3/4 of the building is occupied. That means only 3/4 of all the units are used. To find out how many units are unoccupied, I need to find out what's left after using 3/4.\n",
      "\n",
      "So, if 3/4 is occupied, then the remaining fraction is 1 - 3/4, which is 1/4. Therefore, 1/4 of all units are unoccupied. Let me calculate that: 120 * (1/4) equals 30. So, there are 30 unoccupied units in total.\n",
      "\n",
      "Wait, let me double-check my calculations to make sure I didn't make a mistake. 15 floors times 8 units per floor is indeed 120 units. 3/4 of 120 is 90, which means 90 are occupied and 30 are unoccupied. Yeah, that makes sense.\n",
      "\n",
      "I think that's all there is to it. The answer should be 30.\n",
      "</think>\n",
      "\n",
      "The total number of unoccupied units in the building is **30**.\n",
      "\n",
      "Step-by-step explanation:\n",
      "\n",
      "1. Total units = 15 floors * 8 units/floor = 120 units.\n",
      "2. Occupied units = (3/4) * 120 = 90 units.\n",
      "3. Unoccupied units = 120 - 90 = **30** units.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Ollama LLM\n",
    "llm_deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.7)\n",
    "\n",
    "output = llm_deepseek.invoke(input)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's demonstrate how to use LangChain Expression Language (LCEL) to build a flexible pipeline for the Q&A. This time let's try Llama 3.2 3B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Unoccupied: 5 * 8 = 40', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-03-04T14:07:15.79457Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1631878375, 'load_duration': 44888667, 'prompt_eval_count': 80, 'prompt_eval_duration': 1263000000, 'eval_count': 12, 'eval_duration': 321000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-78c7baa2-f9c8-44b2-bb0d-3f88e72522fc-0', usage_metadata={'input_tokens': 80, 'output_tokens': 12, 'total_tokens': 92})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a prompt template from the template string\n",
    "template = \"Answer the questions briefly and directly with just a few words. \\n\\n{question}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "question = \"Richard lives in an apartment building with 15 floors. Each floor contains 8 units, and 3/4 of the building is occupied. What's the total number of unoccupied units In the building?\"\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm_llama32 = ChatOllama(model=\"llama3.2:3b\", temperature=0.7)\n",
    "\n",
    "# Create the LLM chain\n",
    "llm_chain = prompt | llm_llama32 \n",
    "\n",
    "response = llm_chain.invoke({\"question\": question})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alternatively, we can simply extract the text part in the response by adding one more stage to the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To find the total number of unoccupied units, first calculate the total number of units in the building: \\n\\n15 floors * 8 units/floor = 120 units\\n\\nSince 3/4 of the building is occupied, the remaining 1/4 is unoccupied. \\n\\nTotal unoccupied units = (1/4) * 120\\n= 30 units'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the LLM chain with the text content extractor\n",
    "llm_chain = prompt | llm_llama32 | StrOutputParser()\n",
    "\n",
    "response = llm_chain.invoke({\"question\": question})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you want to display the response text in Markdown format in a pretty way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To find the total number of unoccupied units, first calculate the total number of units in the building: \n",
       "\n",
       "15 floors * 8 units/floor = 120 units\n",
       "\n",
       "Since 3/4 of the building is occupied, the remaining 1/4 is unoccupied. \n",
       "\n",
       "Total unoccupied units = (1/4) * 120\n",
       "= 30 units"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, you know the basics to build AI apps using LangChain. üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spectrum of Reasoning Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Modern LLMs demonstrate a range of reasoning abilities across different domains:\n",
    "\n",
    "- **Arithmetic reasoning**: Solving mathematical problems with calculations\n",
    "- **Math Problem-Solving**: Solving challenging math problems\n",
    "- **Scientific Reasoning**: Making inferences based on scientific domain knowledge\n",
    "- **Commonsense reasoning**: Making inferences based on everyday knowledge\n",
    "- **Lognical Reasoning**: Drawing valid conclusions from premises\n",
    "- **Visual Reasoning**: Making inferences based on everyday knowledge\n",
    "- **etc.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Arithmetic Reasoning: Solving mathematical problems with calculations\n",
    "\n",
    "- Sample: \"Q: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? A: 72\"\n",
    "- Example Dataset: [GSM8K (Grade School Math 8K)](https://huggingface.co/datasets/openai/gsm8k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. Math Problem-Solving: Solving challenging math problems\n",
    "- Sample: \"Q: If $f(x) = \\frac{3x-2}{x-2}$, what is the value of $f(-2) +f(-1)+f(0)$? Express your answer as a common fraction.  A: \\frac{14}{3}\"\n",
    "- Example Dataset: [MATH 500](https://huggingface.co/datasets/HuggingFaceH4/MATH-500)\n",
    "- Sample: \"Q: There exist real numbers $x$ and $y$, both greater than 1, such that $\\log_x\\left(y^x\\right)=\\log_y\\left(x^{4y}\\right)=10$. Find $xy$.  A: 25\"\n",
    "- Example Dataset: [AIME (American Invitational Mathematics Examination) 2024](https://huggingface.co/datasets/Maxwell-Jia/AIME_2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Scientific Reasoning: Making inferences based on scientific domain knowledge\n",
    "- Description: GPQA (Graduate-Level Google-Proof Q&A) is a multiple-choice, Q&A dataset of very hard questions written and validated by experts in biology, physics, and chemistry. When attempting questions out of their own domain (e.g., a physicist answers a chemistry question), these experts get only 34% accuracy, despite spending >30m with full access to Google.\n",
    "- Example Dataset: [GPQA](https://huggingface.co/datasets/Idavidrein/gpqa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4. Commonsense Reasoning: Making inferences based on everyday knowledge\n",
    "- Sample: \"Q: The fox walked from the city into the forest, what was it looking for? Answer Choices: (a) pretty flowers (b) hen house (c) natural habitat (d) storybook  A: (b)\"\n",
    "- Example Dataset: [CommonsenseQA](https://www.tau-nlp.sites.tau.ac.il/commonsenseqa), [HellaSwag](https://github.com/rowanz/hellaswag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5. Logical Reasoning: Drawing valid conclusions from premises\n",
    "- Sample: \n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"imgs/L02_HotpotQA_Sample.png\" alt=\"HotpotQA Sample\" style=\"width:50%; height:auto;\">\n",
    "</div>\n",
    "\n",
    "- Example Dataset: [HotpotQA](https://hotpotqa.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "6. Visual Reasoning: Questions require an understanding of vision, language and commonsense knowledge to answer.\n",
    "- Sample: <img src=\"imgs/L02_VQA_BigBang.png\" alt=\"VQA Sample\" style=\"width:50%; height:auto;\">\n",
    "\n",
    "   - \"Q: What time of day is it?  A: Night\"\n",
    "- Example Dataset: [VQA v2 (Visual Question Answering)](https://visualqa.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SOTA Reasoning Models and Their Approaches to Enhance Reasoning Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recent breakthroughs in artificial intelligence have led to the development of several language models with unprecedented reasoning capabilities, showcasing how innovative training methodologies and architectural choices can significantly improve a model‚Äôs ability to tackle complex problems.\n",
    "\n",
    "Reasoning models, such as OpenAI o1/o3 series and DeepSeek R1, are designed to tackle complex problem-solving tasks, particularly in domains like science, coding, and mathematics. These models go beyond traditional language generation by incorporating structured reasoning processes, where the model breaks down problems into sequential steps before arriving at a final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Below is a list of several prominent ‚Äúreasoning‚Äù‚Äêfocused large language models (LLMs). Note that many of these models emerged during late 2024 into early 2025 as part of an industry‚Äêwide push toward enhanced reasoning capabilities.\n",
    "\n",
    "| **Model**                      | **Release Date**         | **Company/Organisation**          |\n",
    "|--------------------------------|--------------------------|-----------------------------------|\n",
    "| o1‚Äëpreview                     | September 2024           | OpenAI                            |\n",
    "| o1                             | December 2024            | OpenAI                            |\n",
    "| o3‚Äëmini                        | January 2025             | OpenAI                            |\n",
    "| o3‚Äëmini‚Äëhigh                   | January 2025             | OpenAI                            |\n",
    "| Claude 3.7                     | February 2025            | Anthropic                         |\n",
    "| DeepSeek‚ÄëR1                    | January 2025             | DeepSeek     |\n",
    "| Doubao‚Äë1.5‚Äëpro                 | January 2025             | ByteDance          |\n",
    "| Kimi k1.5                      | January 2025*            | Moonshot AI                       |\n",
    "| QwQ‚Äë32B‚ÄëPreview                | December 2024*           | Alibaba Cloud                     |\n",
    "| Grok 3                         | February 2025*           | xAI          |\n",
    "| Gemini 2.0 Flash Thinking Exp. | February 2025*           | Google DeepMind                   |\n",
    "\n",
    "\\* Approximate dates based on available reports and media coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> üí¨ **Discussion:** \n",
    "> \n",
    "> What are the core techniques used to enhance the reasoning capability in the state-of-the-art reasoning models, i.e., OpenAI o1/o3, DeepSeek R1? Extract the core techniques from AI summary.\n",
    ">\n",
    "> ü§ñ Reference prompt: *\"Summarise the core techniques used to enhance the reasoning capability in the state-of-the-art reasoning models, i.e., OpenAI o1/o3, DeepSeek R1, respectively\"*\n",
    ">\n",
    "> üß† Critical Thinking: Is the information verifiable and accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### OpenAI o1/o3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Put the GenAI answer and your notes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### DeepSeek R1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Put the GenAI answer and your notes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, you know the techniques behind the SOTA reasoning models üöÄ. Let's delve deeper into the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prerequisite: A Brief Introduction to Prompt Engineering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Prompt engineering is the practice of crafting effective inputs to guide AI language models toward producing desired outputs. It involves strategically designing questions, instructions, and context to elicit accurate, relevant, and useful responses.\n",
    "\n",
    "Key aspects include:\n",
    "\n",
    "- Structuring prompts with clear instructions and constraints\n",
    "- Using specific formatting techniques\n",
    "- Providing examples to demonstrate the expected response format\n",
    "- Breaking complex tasks into sequential steps\n",
    "- Including relevant context to improve understanding\n",
    "- Setting appropriate tone, style, and level of detail\n",
    "\n",
    "Effective prompt engineering can significantly enhance AI performance across various applications, from content creation and data analysis to problem-solving and creative work. As AI systems evolve, prompt engineering continues to develop as both an art and a science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zero-shot prompting refers to asking an LLM to perform a task without providing specific examples of that task. The model relies solely on its pre-training knowledge.\n",
    "\n",
    "**Key characteristics:**\n",
    "- No examples provided in the prompt\n",
    "- Requires clear instructions\n",
    "- Performance varies greatly with prompt phrasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example:**\n",
    "\n",
    "Prompt:\n",
    "```Text\n",
    "What is the capital of France?\n",
    "```\n",
    "\n",
    "Output:\n",
    "```Text\n",
    "Paris\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Few-shot prompting involves providing the model with a few examples of the task before asking it to perform a similar task. This helps the model understand the expected format and reasoning style.\n",
    "\n",
    "**Key characteristics:**\n",
    "- Includes examples within the prompt\n",
    "- Helps align model output to desired format\n",
    "- Can improve performance on complex tasks\n",
    "- Examples should be representative and diverse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "**Example:** ([Brown et al. 2020](https://arxiv.org/abs/2005.14165))\n",
    "\n",
    "Prompt:\n",
    "```Text\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\n",
    "the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus\n",
    "\n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    "```\n",
    "\n",
    "Output:\n",
    "```Text\n",
    "One day when I was playing tag with my little sister, she got really excited and she\n",
    "started doing these crazy farduddles.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chain-of-Thought (CoT) <a id=\"cot\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Chain-of-Thought (CoT) was introduced by [Wei et al. (2022)](https://arxiv.org/abs/2201.11903) as a prompting technique to enhance the reasoning capabilities of Large Language Models (LLMs), especially in multi-step reasoning tasks.\n",
    "\n",
    "In contrast to the standard prompting, where models are asked to directly produce the final answer, 'Chain of Thought Prompting' encourages LLMs to break down complex problems into intermediate reasoning steps before arriving at the final answer. By doing this, the model-generated 'chain of thought' can mimic an intuitive human thought process when working through multi-step problems.\n",
    "\n",
    "**Key benefits:**\n",
    "- Significantly improves performance on reasoning tasks\n",
    "- Provides transparency into the model's reasoning process\n",
    "- Reduces hallucination by making each step explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Few-Shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![CoT Prompting](imgs/L02_CoT_Prompt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Examples of CoT in different reasoning tasks [(Wei et al., 2022)](https://arxiv.org/abs/2201.11903):\n",
    "\n",
    "![CoT Example Tasks](imgs/L02_CoT_ExampleTasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Zero-Shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Kojima et al. (2022)](https://arxiv.org/abs/2205.11916) introduce a simplified approach by appending the words \"Let's think step by step.\" to the end of a question. This simple prompt helps the LLM to generate a chain of thought that answers the question, from which the LLM can then extract a more accurate answer.\n",
    "\n",
    "![Zero-Shot CoT](imgs/L02_ZeroShotCot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üßë‚Äçüè´ Demo: Reasoning by CoT Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Standard Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's start with a standard prompt as the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the question that will be reused for different prompts\n",
    "question = \"Four friends ordered four pizzas for a total of 64 dollars. If two of the pizzas cost 30 dollars, how much did each of the other two pizzas cost if they cost the same amount?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create a prompt template for standard prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Initialise a chat model using the Ollama LLM\n",
    "\n",
    "\n",
    "# TODO: Create the LLM chain with the standard prompt template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Run the chain with the question to get the string output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Zero-shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Zero-shot Chain-of-Thought involves explicitly asking the model to reason **step by step**, without providing examples of step-by-step reasoning. This is often triggered by adding phrases like `Let's think step by step` to prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this demo, let's try to use `ChatPromptTemplate` to create the prompt, so that you can use a system prompt to define the behaviour of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create a zero-shot CoT prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Initialise a chat model using the Ollama LLM\n",
    "\n",
    "\n",
    "# TODO: Create the LLM chain with the zero-shot prompt template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Run the chain with the question to get the string output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Few-shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Few-shot Chain-of-Thought combines the benefits of few-shot prompting and Chain-of-Thought reasoning. By providing examples that demonstrate step-by-step reasoning, which helps the model understand how to break down problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's firstly define the example Q&A that can be integrated in the few-shot prompt template. (6 shots selected from the Appendix G in [(Wei et al., 2022)](https://arxiv.org/abs/2201.11903))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example_qa = \"\"\"\n",
    "Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "Answer: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "Answer: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "Answer: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\n",
    "\n",
    "Question: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
    "Answer: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\n",
    "\n",
    "Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
    "Answer: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\n",
    " \n",
    "Question: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "Answer: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create a few-shot CoT prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Initialise a chat model using the Ollama LLM\n",
    "\n",
    "\n",
    "# TODO: Create the LLM chain with the few-shot prompt template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Run the chain with the question to get the string output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> üí° Note:\n",
    "> Through this demo, you should realise the benefit of using LangChain prompt templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> üí¨ **Discussion:** \n",
    "> \n",
    "> Why the few-shot CoT prompting doesn't work in this example? Will the response change if we run multiple times? What if we change the temperature?\n",
    ">\n",
    "> üß† Critical Thinking: What are the limitations of CoT? How is the CoT technique incorporated in the reasoning models without requiring users to write CoT prompts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations of Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While Chain-of-Thought techniques provide significant improvements in reasoning capabilities, they also come with several important limitations:\n",
    "\n",
    "- **Reasoning Hallucinations**: \n",
    "  - LLMs may produce plausible-sounding but incorrect reasoning steps. These \"hallucinated\" steps can lead to wrong conclusions while appearing confident and logical.\n",
    "  - Example: When solving a complex physics problem, the model might introduce physically impossible intermediate calculations that seem reasonable but violate fundamental laws.\n",
    "\n",
    "- **Sensitivity to Prompt Wording**:\n",
    "  - CoT performance is highly dependent on how the prompt is phrased. Small changes in wording can lead to different reasoning paths and outcomes.\n",
    "  - Finding optimal prompts often requires extensive experimentation and may not generalize across different problem types.\n",
    "\n",
    "- **Computational Overhead**:\n",
    "  - CoT reasoning requires significantly more tokens than direct prompting, increasing:\n",
    "    - API costs when using commercial LLMs\n",
    "    - Latency in generating responses\n",
    "    - Computational resources needed\n",
    "\n",
    "- **Dependence on Example Quality in Few-shot CoT**\n",
    "  - The performance of few-shot CoT heavily depends on:\n",
    "    - The quality and relevance of chosen examples\n",
    "    - The similarity between examples and target problems\n",
    "    - The order in which examples are presented\n",
    "\n",
    "- **Limited Self-correction**:\n",
    "  - When a reasoning path leads to an error, LLMs often struggle to identify and correct the mistake, instead continuing with flawed reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self Consistency Chain of Thought\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An improvement on CoT prompting called \"Self Consistency\" is proposed by [Wang et al. (2022)](https://arxiv.org/abs/2203.11171). Self-consistency aims \"to replace the naive **greedy decoding** used in chain-of-thought prompting\". This approach **samples** multiple, diverse reasoning paths based on few-shot CoT, then select the most consistent answer among all reasoning paths. The evaluation shows it \"boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![SC CoT](imgs/L02_CoT_SC.png)\n",
    "\n",
    "> ü§ñ Implementation with AI\n",
    "> Based on the description of the Self-Consistency idea, think about how to implement it with the help of AI tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the SC-CoT algorithm using local open-weight/source LLMs and LCEL (preferablly parallel prompting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An example of multi-path response (some parts are omitted due to the lengthy response):\n",
    "\n",
    "{'path_1': \"To find out how much each of the other two pizzas cost, let's break down what we know:\\n\\n1. Total cost for four pizzas: $64\\n2. Cost for two pizzas: $30\\n\\nLet's denote the price of one pizza as \\\\(x\\\\).\\n\\nSo, the equation representing the total cost would be:\\n\\\\[2x + 2x = 30\\\\]\\nSimplifying this gives us:\\n\\\\[4x = 30\\\\]\\n\\nTo find out how much each of the other two pizzas cost (\\\\(2x\\\\) since there are two), divide both sides by \\\\(2\\\\):\\n\\\\[x = \\\\frac{30}{2}\\\\]\\n\\\\[x = 15\\\\]\\n\\nSo, each of the other two pizzas costs $15.\", \n",
    "\n",
    "'path_2': \"To solve this problem, ... Therefore, each of the remaining two pizzas cost \\\\$17.\", \n",
    "\n",
    "'path_3': \"Let's break down the problem step-by-step.... Step 6: Present the final answer.\\n- Each of the other two pizzas costs $17.\\n\\nTherefore, the answer is $17.\", \n",
    "\n",
    "'path_4': ... Therefore, each of the other two pizzas cost $17.\", \n",
    "\n",
    "'path_5': 'To find out how much each of the other two pizzas cost ... Therefore, each of the two remaining pizzas cost $1.33.'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since multiple LLM responses must be sampled, the computational cost and response latency will be higher than the typical Chain of Thought (CoT) approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> üí¨ **Discussion:** \n",
    "> \n",
    "> üß† Critical Thinking: CoT technique can be embedded into a reasoning model by exposing a base model to CoT data in training/fine-tuning, which is called **train-time compute**. Is it reasonable to integrate SC-CoT into a model during training/fine-tuning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> üí° Note:\n",
    "> When we sample multiple reasoing path in SC-CoT, the model will use longer time to think, i.e., compute for longer time during the test time. So, SC-CoT can be considered as a basic technique for **test-time compute**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation and Benchmarking\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![OpenAI o1 Benchmarks](./imgs/L01_O1_Benchmarks.png)\n",
    "\n",
    "Source: [OpenAI, Learning to Reason with LLMs](https://openai.com/index/learning-to-reason-with-llms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![OpenAI o1 Benchmarks](./imgs/L01_DeepSeek_Benchmarks.png)\n",
    "\n",
    "Source: DeepSeek R1 paper ([DeepSeek-AI, 2024](https://arxiv.org/abs/2501.12948))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluating reasoning capabilities of LLMs requires specialized benchmarks and metrics. Here we discuss some common approaches:\n",
    "\n",
    "**Common reasoning benchmarks:**\n",
    "- [GSM8K](https://huggingface.co/datasets/openai/gsm8k): Grade School Math problems requiring multi-step reasoning\n",
    "- [MATH 500](https://huggingface.co/datasets/HuggingFaceH4/MATH-500): A subset of 500 problems from the MATH benchmark that OpenAI created in their Let's Verify Step by Step paper.\n",
    "- [AIME 2024](https://huggingface.co/datasets/Maxwell-Jia/AIME_2024): This dataset contains problems from the American Invitational Mathematics Examination (AIME) 2024. AIME is a prestigious high school mathematics competition known for its challenging mathematical problems.\n",
    "- [GPQA Diamond](https://huggingface.co/datasets/Idavidrein/gpqa): The GPQA Diamond subset is a higher-quality, more challenging subset of the main GPQA dataset. It contains 198 questions for which both domain expert annotators got the correct answers, but which the majority of non-domain experts answered incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Other benchmarks**\n",
    "- [MMLU](https://huggingface.co/datasets/Stevross/mmlu): Massive Multitask Language Understanding is a massive multitask test consisting of multiple-choice questions from various branches of knowledge. The test spans subjects in the humanities, social sciences, hard sciences, and other areas that are important for some people to learn. This covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability.\n",
    "- [CodeForces](https://huggingface.co/datasets/open-r1/codeforces): CodeForces is one of the most popular websites among competitive programmers, hosting regular contests where participants must solve challenging algorithmic optimization problems. The challenging nature of these problems makes them an interesting dataset to improve and test models‚Äô code reasoning capabilities.\n",
    "- [SWE-Bench](https://www.swebench.com/): A benchmark for evaluating large language models‚Äô (LLMs‚Äô) abilities to solve real-world software issues sourced from GitHub. The benchmark involves giving agents a code repository and issue description, and challenging them to generate a patch that resolves the problem described by the issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pass@k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In LLM evaluations, pass@k is a metric used to assess a model's ability to generate correct solutions (e.g., code, answers) by considering multiple attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> üí¨ **Discussion:** \n",
    "> \n",
    "> Try to learn from GenAI what \"pass@k\" means in LLM evaluations and use resources to find out whether GenAI is 100% correct.\n",
    ">\n",
    "> ü§ñ Reference prompt: *\"what does \"pass@k\" mean in LLM evaluations?\"*\n",
    ">\n",
    "> üß† Critical Thinking: Does the GenAI provide sources? Is the answer coherent? Is it reasonable when apply the definition to pass@1? Does it mentioned different ways to compute pass@k? Have you verified the responses from the GenAI you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Put GenAI answers and your notes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The pass@k metric measures the probability, computed over a set of problems, that at least one of the top $k$ generated outputs for each problem contains the correct solution.\n",
    "\n",
    "For example, a Pass@1 of 30% and a Pass@10 of 60% would mean that the model has a 30% chance of solving the problem on the first try, but a 60% chance of finding a correct solution if allowed to generate 10 different attempts.\n",
    "\n",
    "- [Kulal et al. (2019)](https://proceedings.neurips.cc/paper/2019/file/7298332f04ac004a0ca44cc69ecf6f6b-Paper.pdf) evaluate functional correctness using the pass@k metric, where k samples are generated per problem, a problem is considered solved if any sample passes the unit tests, and the total fraction of problems solved is reported.\n",
    "- In practice, computing pass@k in this way can have high variance. For example, if we compute pass@1 from a single completion per problem, we can get significantly different values from repeated evaluations due to sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- [OpenAI (2021)](https://arxiv.org/abs/2107.03374) introduced an unbiased estimator that accounts for the total number of generated samples $n$, the number of correct samples $c$, and the desired $k$ value. To evaluate pass@k,\n",
    "  - generate $n \\geq k$ samples per problem/task\n",
    "  - count the number of correct samples $c \\leq n$\n",
    "  - calculate the unbiased estimator:\n",
    "\n",
    "$$\n",
    "\\text{pass@k} = \\mathbb{E}_{\\text{problems}} \\left[ 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- When calculate pass@1, [DeepSeek-AI, (2024)](https://arxiv.org/abs/2501.12948) uses a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question and calcuates the pass@1 metric for their DeepSeek R1 model as:\n",
    "\n",
    "$$\n",
    "\\text{pass@k} = \\frac{1}{k} \\sum_{i=1}^k p_i\n",
    "$$\n",
    "\n",
    "where $p_i$ denotes the correctness of the $i$-th response.\n",
    "\n",
    "- For OpenAI o1 benchmark, they calculate pass@1 in the same way, as indicated by [OpenAI, (2024)]():\n",
    "> All models are given 5 tries to generate a candidate patch. We compute pass@1 by averaging the per-instance pass rates of all samples that generated a valid (i.e., non-empty) patch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**\n",
    "Suppose one coding problem is evaluated with 100 samples ($n$=100), and 3 are correct ($c$=3):\n",
    "\n",
    "pass@1: 3 / 100 = 3% (probability a single random guess is correct).\n",
    "\n",
    "pass@3: $1 ‚àí \\frac{\\binom{100-3}{3}}{\\binom{100}{3}} \\approx 8.8\\%$\n",
    "\n",
    "pass@100: 100% (if at least one correct answer exists in 100 samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### cons@x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consensus or majority vote, denoted as cons@x or maj@x, measures whether the most frequent answer (majority vote) among x generated responses is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Computational Cost: Generating many samples (e.g., n=100 n=100) is resource-intensive.\n",
    "- Domain-Specific: Works best for tasks with clear correctness criteria (e.g., code, math)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reasoning techniques for LLMs have numerous practical applications across various domains:\n",
    "\n",
    "**Educational applications:**\n",
    "- Step-by-step math problem solving\n",
    "- Scientific reasoning and explanation\n",
    "- Tutoring with transparent reasoning\n",
    "\n",
    "**Business and finance:**\n",
    "- Financial analysis and planning\n",
    "- Risk assessment\n",
    "- Decision support systems\n",
    "\n",
    "**Healthcare:**\n",
    "- Diagnostic reasoning assistance\n",
    "- Treatment plan evaluation\n",
    "- Medical literature analysis\n",
    "\n",
    "**Software development:**\n",
    "- Code generation with explanation\n",
    "- Debugging assistance\n",
    "- Algorithm design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this notebook, we explored various reasoning techniques for LLMs,from Chain-of-Thought to Self Consistency to enhance problem-solving capabilities. Additionally, we cover evaluation metrics such as pass@k and cons@x, which are crucial for assessing the performance of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Key Takeaways**\n",
    "1. **Reasoning Capabilities**: Modern LLMs demonstrate a range of reasoning abilities, including arithmetic, scientific, commonsense, logical, and visual reasoning.\n",
    "2. **Chain-of-Thought (CoT)**: CoT prompting significantly improves the reasoning performance of LLMs by breaking down complex problems into intermediate steps.\n",
    "3. **Self Consistency CoT**: Enhances CoT by sampling multiple reasoning paths and selecting the most consistent answer, further improving accuracy.\n",
    "4. **Evaluation Metrics**: Metrics like pass@k and cons@x are essential for evaluating the reasoning capabilities of LLMs.\n",
    "5. **Practical Applications**: Reasoning techniques have wide-ranging applications in education, business, healthcare, and software development, enhancing the utility and reliability of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reasoning techniques continue to evolve rapidly, enabling LLMs to tackle increasingly complex problems with greater reliability. By understanding and implementing these techniques, you can significantly enhance the capabilities of LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bonus Scene\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the project diretory, run the `eight_puzzle.py` to play the eight puzzle game.\n",
    "\n",
    "```python\n",
    "python eight_puzzle.py\n",
    "```\n",
    "\n",
    "Can you solve the preset puzzle? How many steps it takes you to solve the puzzle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your notes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> üí¨ **Discussion:** \n",
    "> \n",
    "> Try to prompt the best reasoning LLMs (those you have access to) to solve the puzzle.\n",
    ">\n",
    "> ü§ñ Reference prompt: \n",
    "> ```text\n",
    "> please solve the 8-puzzle below, where 0 represents the blank tile. Please provide your thoughts about how to solve the problem and the solution step-by-step\n",
    "> [4, 1, 3],\n",
    "> [0, 8, 5],\n",
    "> [2, 7, 6]\n",
    "> ```\n",
    ">\n",
    "> üß† Critical Thinking: Can the smartest GenAI solve the puzzle only by the CoT reasoning? Can the GenAI models or you find the fewest steps to solve the puzzle only the the CoT reasoning? Check out the thougths generated by the reasoning model, do they mention any method to solve the puzzle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DeepSeek thought for over 10 min! Unfortunately, it didn't solve the puzzle.\n",
    "\n",
    "![DeepSeek Thinking for 8-puzzle](./imgs/L01_BonusScene_DeepSeekThinking.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
